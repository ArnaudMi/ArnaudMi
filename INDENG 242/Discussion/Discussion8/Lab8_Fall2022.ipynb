{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import odr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Boostrap basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "normal_data = np.random.normal(4, 1.5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the best of the GSIs' knowledge, there is no perfect package for boostrap validation in `Python`. Therefore, the function below is a simple manual implementation of the bootstrap. Note that there is nothing tricky in the code below and you are encouraged to try the implementation on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, metrics,sample =500, random_state=10):\n",
    "    output_array=np.zeros(sample)\n",
    "    # output_array[:]=np.nan\n",
    "    for i in range(sample):\n",
    "        bs_data = np.random.choice(data, len(data), replace=True)\n",
    "        output_array[i] = metrics(bs_data)\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_out = bootstrap(normal_data, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
    "axs[0].set_xlabel('Data observation', fontsize=16)\n",
    "axs[1].set_xlabel('Mean estimate', fontsize=16)\n",
    "axs[0].set_ylabel('Count', fontsize=16)\n",
    "axs[0].hist(normal_data,bins=30,edgecolor='red', linewidth=2,color = \"grey\")\n",
    "axs[1].hist(boot_out,bins=30,edgecolor='blue', linewidth=2,color = \"grey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bootstrap output, you can compute the stiatistics (e.g., mean, standard deviation, quantiles) of the boostrap estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean of the estimate %.2f\" % np.mean(boot_out))\n",
    "print(\"sd of the estimate: %.2f \" % np.std(boot_out))\n",
    "print(\"0.025-quantile of the estimate: %.2f \" % np.quantile(boot_out, 0.025))\n",
    "print(\"0.975-quantile of the estimate: %.2f \" % np.quantile(boot_out, 0.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap for Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 From Lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = pd.read_csv(\"CTR.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = ctr['CTR']\n",
    "X = pd.get_dummies(ctr.drop(['CTR'], axis=1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=88)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OSR2(model, X_test, y_test, y_train):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    SSE = np.sum((y_test - y_pred)**2)\n",
    "    SST = np.sum((y_test - np.mean(y_train))**2)\n",
    "                 \n",
    "    return (1 - SSE/SST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we take the random forest model as an example; However, bootstrap for valiation can be applied to any trained model.\n",
    "\n",
    "Below, we train the random forest model, on the training data and with the best parameters found through cross-validation in Lab 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_features=11, min_samples_leaf=5, \n",
    "                           n_estimators = 500, random_state=10, verbose=1)\n",
    "# Note: you can change the verbose parameter to control how much training progress is printed.\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf.verbose = False\n",
    "print('OSR2:', round(OSR2(rf, X_test, y_test, y_train), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Define metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OS_R_squared(predictions, y_test,y_train):\n",
    "    SSE = np.sum((y_test-predictions)**2)\n",
    "    SST = np.sum((y_test-np.mean(y_train))**2)\n",
    "    r2 = 1-SSE/SST\n",
    "    return r2\n",
    "\n",
    "def mean_squared_error(predictions, y_test,y_train):\n",
    "    MSE = np.mean((y_test-predictions)**2)\n",
    "    return MSE\n",
    "\n",
    "def mean_absolute_error(predictions, y_test,y_train):\n",
    "    MAE = np.mean(np.abs(y_test-predictions))\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "print(\"OSR2: %s\" % OS_R_squared(y_pred,y_test,y_train))\n",
    "print(\"MSE: %s\" % mean_squared_error(y_pred,y_test,y_train))\n",
    "print(\"MAE: %s\" % mean_absolute_error(y_pred,y_test,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"OSR2: %s\" % OSR2(rf, X_test, y_test,y_train))\n",
    "print(\"MSE: %s\" % np.mean((y_pred-y_test)**2))\n",
    "print(\"MAE: %s\" % np.mean(np.abs(y_pred-y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Boostrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Manually code the bootstrap method\n",
    "\n",
    "Again, below is a manual implementation of bootstrap for model valiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def bootstrap_validation(test_data, test_label, train_label, model, metrics_list, sample=500, random_state=66):\n",
    "    tic = time.time()\n",
    "    n_sample = sample\n",
    "    n_metrics = len(metrics_list)\n",
    "    output_array=np.zeros([n_sample, n_metrics])\n",
    "    output_array[:]=np.nan\n",
    "    print(output_array.shape)\n",
    "    for bs_iter in range(n_sample):\n",
    "        bs_index = np.random.choice(test_data.index, len(test_data.index), replace=True)\n",
    "        bs_data = test_data.loc[bs_index]\n",
    "        bs_label = test_label.loc[bs_index]\n",
    "        bs_predicted = model.predict(bs_data)\n",
    "        for metrics_iter in range(n_metrics):\n",
    "            metrics = metrics_list[metrics_iter]\n",
    "            output_array[bs_iter, metrics_iter]=metrics(bs_predicted,bs_label,train_label)\n",
    "#         if bs_iter % 100 == 0:\n",
    "#             print(bs_iter, time.time()-tic)\n",
    "    output_df = pd.DataFrame(output_array)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Do bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we use B = 5,000, instead of 10,000 as shown in the lecture. Later, you will observe in the histogram that with this resampling size, the estimates of the metrics have already displayed the nice normal curve shape.\n",
    "\n",
    "The following code takes about 10 minutes to run. You might want to reduce the value of B for a trial run. However, B should not be too small; otherwise, the estimates may not converge to the standard normal distribution well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_output = bootstrap_validation(X_test,y_test,y_train,rf,\n",
    "                                 metrics_list=[OS_R_squared, mean_squared_error,mean_absolute_error],\n",
    "                                 sample = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Basic plot and centered plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_OSR2 = OS_R_squared(y_pred,y_test,y_train)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
    "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
    "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
    "axs[0].set_ylabel('Count', fontsize=16)\n",
    "axs[0].hist(bs_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
    "axs[0].set_xlim([0.4,0.7])\n",
    "axs[1].hist(bs_output.iloc[:,0]-test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
    "axs[1].set_xlim([-0.15,0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 manual CI + plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 95% confidence interval\n",
    "CI= np.quantile(bs_output.iloc[:,0]-test_OSR2,np.array([0.025,0.975]))\n",
    "print(\"The 95-percent confidence interval of OSR2 is %s\" % CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1, figsize=(8,6))\n",
    "axs.set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
    "axs.set_ylabel('Count', fontsize=16)\n",
    "axs.hist(bs_output.iloc[:,0]-test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
    "axs.set_xlim([-0.15,0.15])\n",
    "axs.vlines(x=CI[0], ymin = 0, ymax =800, color = \"black\")\n",
    "axs.vlines(x=CI[1], ymin = 0, ymax =800, color = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to re-do this exercise with another models that we did in Lab 7, e.g., linear regression, decision tree, and bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea behind boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen with Bagging and Random Forest how to use the power of model ensemble to enhance model performance. Another interesting idea is adaptively combined simple models. How?\n",
    "1. Start simple, use linear regression for a regression task --> h_1(X). \n",
    "2. Take the training set response and subtract the predicted value R_1(X) = y - h_1(X).\n",
    "3. Fit another easy model, i.e a square model to the residual R_1 --> h_2(X)\n",
    "4. Repeat until a stopping criterion is met\n",
    "\n",
    "This is powerful because is numerically better than running the regression on [1, X, X^2] and this is because, instead of the Vandermonde matrix, we use a basis change to obtain orthogonal basis fro the same space (--> orthogonal means no problem with multicollinearity!!!!) \n",
    "\n",
    "In class, you have seen a nice example with a 4th degree polynomial, we are gonna reproduce it and show another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 5.0)\n",
    "y = np.sin(x)\n",
    "poly_model = odr.polynomial(3)  # using third order polynomial model\n",
    "data = odr.Data(x, y)\n",
    "odr_obj = odr.ODR(data, poly_model)\n",
    "output = odr_obj.run()  # running ODR fitting\n",
    "poly = np.poly1d(output.beta[::-1])\n",
    "poly_y = poly(x)\n",
    "plt.plot(x, y, label=\"input data\")\n",
    "plt.plot(x, poly_y, label=\"polynomial ODR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 5.0)\n",
    "y = np.sin(x)\n",
    "poly_model = odr.polynomial(3)  # using third order polynomial model\n",
    "data = odr.Data(x, y)\n",
    "odr_obj = odr.ODR(data, poly_model)\n",
    "output = odr_obj.run()  # running ODR fitting\n",
    "poly = np.poly1d(output.beta[::-1])\n",
    "poly_y = poly(x)\n",
    "plt.plot(x, y, label=\"input data\")\n",
    "plt.plot(x, poly_y, label=\"polynomial ODR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x = np.linspace(-2, 4, 100).reshape(-1, 1)\n",
    "state = np.random.seed(10)\n",
    "eps = np.random.normal(0, 5, size=(100, 1))\n",
    "true_y = x**4 - 3*x**3 - 2*x**2 +5*x + 10\n",
    "y = x**4 - 3*x**3 - 2*x**2 +5*x + 10 + eps\n",
    "plt.plot(x, true_y, label=\"input data\")\n",
    "plt.scatter(x, y, label=\"noisy data\", color='orange')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression().fit(x, y)\n",
    "intercept_1 = model1.intercept_\n",
    "coefficients_1 = model1.coef_\n",
    "prediction_1 = model1.predict(x)\n",
    "residuals_1 = y - prediction_1\n",
    "print('Intercept:', intercept_1)\n",
    "print('Coefficients:', coefficients_1)\n",
    "plt.plot(x, true_y, label=\"input data\")\n",
    "plt.plot(x, intercept_1 + coefficients_1*x, label=\"linear regression\")\n",
    "plt.scatter(x, residuals_1, label=\"residual after the first regression\", color = 'g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((x, x**2))\n",
    "model2 = LinearRegression().fit(X, residuals_1)\n",
    "intercept_2 = model2.intercept_\n",
    "coefficients_2 = np.array(model2.coef_.reshape(-1, 1))\n",
    "prediction_2 = model2.predict(X)\n",
    "residuals_2 = residuals_1 - prediction_2\n",
    "print('Intercept:', intercept_2)\n",
    "print('Coefficients:', coefficients_2)\n",
    "plt.plot(x, true_y, label=\"input data\")\n",
    "plt.plot(x, residuals_1, label=\"residual for the first regression\")\n",
    "plt.plot(x, intercept_2 + coefficients_2[0]*X[:,0] + coefficients_2[1]*X[:,1], label=\"quadratic regression on r1\")\n",
    "plt.scatter(x, residuals_2, label=\"residual after the second regression\", color='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((x, x**2, x**3))\n",
    "model3 = LinearRegression().fit(X, residuals_2)\n",
    "intercept_3 = model3.intercept_\n",
    "coefficients_3 = np.array(model3.coef_.reshape(-1, 1))\n",
    "prediction_3 = model3.predict(X)\n",
    "residuals_3 = residuals_2 - prediction_3\n",
    "print('Intercept:', intercept_3)\n",
    "print('Coefficients:', coefficients_3)\n",
    "plt.plot(x, true_y, label=\"input data\")\n",
    "plt.plot(x, intercept_3 + coefficients_3[0]*X[:,0] + coefficients_3[1]*X[:,1] + coefficients_3[2]*X[:,2], label=\"Degree 3 regression on r2\")\n",
    "plt.scatter(x, residuals_3, label=\"residual after the third regression\", color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((x, x**2, x**3, x**4))\n",
    "model4 = LinearRegression().fit(X, residuals_3)\n",
    "intercept_4 = model4.intercept_\n",
    "coefficients_4 = np.array(model4.coef_.reshape(-1, 1))\n",
    "prediction_4 = model4.predict(X)\n",
    "residuals_4 = residuals_3 - prediction_4\n",
    "print('Intercept:', intercept_4)\n",
    "print('Coefficients:', coefficients_4)\n",
    "plt.plot(x, true_y, label=\"input data\")\n",
    "plt.plot(x, intercept_4 + coefficients_4[0]*X[:,0] + coefficients_4[1]*X[:,1] + coefficients_4[2]*X[:,2]\n",
    "         + coefficients_4[3]*X[:,3], label=\"Degree 4 regression on r3\")\n",
    "plt.scatter(x, residuals_4, label=\"residual after fourth regression\", color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = prediction_1+prediction_2+prediction_3+prediction_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, true_y, label=\"input data\")\n",
    "plt.scatter(x, final_prediction, label=\"ensemble prediction\", color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "27c0491bc2de4eade874e0d01d7fedd2396e409c6f5607118d2facfa41a47803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
